{
 "cells": [
  {
   "cell_type": "code",
   "id": "a83167d28a72b063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T10:38:36.760481Z",
     "start_time": "2026-01-19T10:38:05.707949Z"
    }
   },
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from utils.llmclass import RAGGenerator\n",
    "from utils.search import Search\n",
    "\n",
    "# Cleanup\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Configuration\n",
    "DB_PATH = os.path.join(os.getcwd(), \"arxiv\")  # Safe path joining\n",
    "# MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d4d85b764a67dbe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T10:38:53.881509Z",
     "start_time": "2026-01-19T10:38:39.938156Z"
    }
   },
   "source": [
    "# Initialize Client & Embedding\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "collection = client.get_or_create_collection(name=\"Arxiv-Database\",\n",
    "                                             embedding_function=embedding_func)\n",
    "search = Search.get(collection)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "6cf7e96393738f81",
   "metadata": {},
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     quantization_config=bnb_config,\n",
    "#     # device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1f358ef76bac712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T10:39:00.942750Z",
     "start_time": "2026-01-19T10:38:58.959120Z"
    }
   },
   "source": [
    "query = \"What is the Assessment Misalignment problem in Large Language Models?\"\n",
    "docs = search(query)\n",
    "\n",
    "print(\"--- Retrieved Context ---\")\n",
    "for doc in docs:\n",
    "    print(f\"Content \\n: {doc['content']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved Context ---\n",
      "Content \n",
      ": the reasoning ability of LLMs by alleviating the assessment misalignment problem caused by VFT.\n",
      "To address the assessment misalignment problem, in this paper, we propose an alignment fine-tuning\n",
      "(AFT) paradigm to improve LLM reasoning with three steps: 1) fine-tuning LLMs using COT\n",
      "training data; 2) generating multiple COT responses for each question using the fine-tuned LLMs,\n",
      "and categorizing them as positive and negative based on whether they deduce the correct answer;\n",
      "3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint\n",
      "alignment (CA) loss. Specifically, the CA loss ensures that all positive scores (the scores of positive\n",
      "COTs) are larger than negative scores. In addition, the negative scores are protected by a constraint\n",
      "term, which is proven to be very important in preventing model degradation. Beyond just binary\n",
      "positive and negative feedback, the CA loss can be seamlessly adapted to ranking situations when\n",
      "Content \n",
      ": which has been overlooked by these approaches, is also crucial for their performance. Extensive\n",
      "experiments on four reasoning benchmarks demonstrate the effectiveness of AFT. In addition, AFT\n",
      "also performs well in multi-task and out-of-distribution situations.\n",
      "8 L IMITATIONS\n",
      "Our paper has some limitations, which should be discussed in future works: 1) Due to the resource\n",
      "limit, we do not scale the AFT to larger LLMs such as 65B and 70B LLama models. However, we\n",
      "believe that larger models still suffer from the assessment misalignment problem of VFT, and thus\n",
      "AFT can improve the performance of these larger models; 2) Our boundary constraint alignment loss\n",
      "incorporates a hyper-parameter β that regulates the constraint strength, significantly impacting the\n",
      "model’s performance. Finding the optimal hyper-parameter requires constructing a validation set\n",
      "and a certain search overhead. Although our detached alignment loss can mitigate the assessment\n",
      "Content \n",
      ": fine-tuned LLMs suffer from an Assessment Misalignment problem, i.e., they\n",
      "frequently assign higher scores to subpar COTs, leading to potential limitations\n",
      "in their reasoning abilities. To address this problem, we introduce an Alignment\n",
      "Fine-Tuning (AFT)paradigm, which involves three steps: 1) fine-tuning LLMs with\n",
      "COT training data; 2) generating multiple COT responses for each question, and\n",
      "categorizing them into positive and negative ones based on whether they achieve\n",
      "the correct answer; 3) calibrating the scores of positive and negative responses\n",
      "given by LLMs with a novel constraint alignment loss. Specifically, the constraint\n",
      "alignment loss has two objectives: a) Alignment, which guarantees that positive\n",
      "scores surpass negative scores to encourage answers with high-quality COTs;\n",
      "b) Constraint, which keeps the negative scores confined to a reasonable range\n",
      "to prevent the model degradation. Beyond just the binary positive and negative\n",
      "Content \n",
      ": Preprint\n",
      "indicate that AFT not only improves the performance of in-distribution tasks but also enhances the\n",
      "model’s transfer ability, leading to significantly better out-of-distribution performance.\n",
      "7 C ONCLUSION\n",
      "In this paper, we find that the vanilla fine-tuned (VFT) LLMs with chain-of-thought (COT) reasoning\n",
      "process suffer from an assessment misalignment problem, i.e, they fail to access the quality of different\n",
      "COTs of the learned questions, which hinders the reasoning ability of LLMs. To this end, we propose\n",
      "an alignment fine-tuning (AFT) paradigm. Our AFT consists of a novel constraint alignment loss that\n",
      "can align the model assessment behaviors without harming the model performance. Furthermore, we\n",
      "also delve deeply into recent widely used ranking losses for alignment and find that the constraint,\n",
      "which has been overlooked by these approaches, is also crucial for their performance. Extensive\n",
      "Content \n",
      ": correct answer. In this paper, we find that previous vanilla fine-tuning (VFT) paradigm causes LLMs\n",
      "to suffer from an Assessment Misalignment problem, i.e., LLMs struggle with accessing the quality\n",
      "1\n",
      "arXiv:2309.02144v1  [cs.CL]  5 Sep 2023\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T10:41:47.466184Z",
     "start_time": "2026-01-19T10:41:47.401273Z"
    }
   },
   "cell_type": "code",
   "source": "rag = RAGGenerator()",
   "id": "23a4a34ab0059bbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.OpenAI object at 0x78122c17aa50>\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T10:42:42.150896Z",
     "start_time": "2026-01-19T10:42:20.440616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. SIMULATE RETRIEVAL\n",
    "query = \"How does the 'Constraint' objective in AFT prevent model degradation?\"\n",
    "docs = search(query)\n",
    "context = [\"-> \".join([doc['source'], doc['content']]) for doc in docs]\n",
    "# 2. GENERATE RESPONSE\n",
    "print(f\"Query: {query}\\n\")\n",
    "answer = rag.generate_answer(query, context)\n",
    "print(f\"Response:\\n{answer}\")"
   ],
   "id": "b6b3aa4b1bd318ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does the 'Constraint' objective in AFT prevent model degradation?\n",
      "\n",
      "Response:\n",
      "The Constraint objective keeps the negative scores confined to a reasonable range via a constraint term in the CA loss, which is proven to be very important in preventing model degradation.\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
