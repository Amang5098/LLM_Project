{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyTzTthNA3t8",
        "outputId": "52576135-ea9f-4811-f787-89cdc67e8ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.2.6)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (0.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community\n",
        "!pip install pypdf\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=850,\n",
        "    chunk_overlap=120\n",
        ")\n",
        "\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "987poR5LBVp3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Securely set your Cerebras AI API key using Colab's secrets management\n",
        "# Click the 'üîë' icon on the left panel, add a new secret named 'CEREBRAS_AI_KEY'\n",
        "# and paste your Cerebras AI API key there.\n",
        "from google.colab import userdata\n",
        "os.environ[\"CEREBRAS_AI_KEY\"] = userdata.get(\"CEREBRAS_AI_KEY\")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-oss-120b\", # Replace with an actual Cerebras AI model name if available\n",
        "    base_url=\"https://api.cerebras.ai/v1\",\n",
        "    api_key=os.environ[\"CEREBRAS_AI_KEY\"]\n",
        ")\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=\"\"\"Your task is to generate one factual question and its ground truth answer STRICTLY from the provided context. Do NOT hallucinate. Ensure the question and answer are verbose, detailed, and directly contextual to the provided text. Ignore any non-factual filler like '----------------------------------'. Return the output as a JSON object with the following keys: 'question', 'contexts', 'answer', and 'ground_truth'.\n",
        "- 'question' should be a well-formulated, verbose question based on the context.\n",
        "- 'contexts' should be a list containing the provided context.\n",
        "- 'answer' should be a comprehensive, detailed ground truth answer, directly extracted or summarized from the context.\n",
        "- 'ground_truth' should be the same as 'answer', representing the comprehensive ground truth.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Return JSON:\n",
        "\n",
        "{{\n",
        "  \"question\": \"...\",\n",
        "  \"contexts\": [...],\n",
        "  \"answer\": \"...\",\n",
        "  \"ground_truth\": \"...\"\n",
        "}}\"\"\"\n",
        ")\n",
        "\n",
        "chain = prompt | llm | JsonOutputParser()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cuLeLIs3BhiS",
        "outputId": "651ea88b-a402-49f0-8b6a-61a425526acf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.7)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.6 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.2.6)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.14.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.13.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "golden_data = []\n",
        "\n",
        "# Redundant code, Q&A generation is handled by generate_qa_dataset.py script"
      ],
      "metadata": {
        "id": "kJg43fPcB5Vz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "874441cd"
      },
      "source": [
        "import json\n",
        "\n",
        "# Redundant code, Q&A generation is handled by generate_qa_dataset.py script"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ec684a1",
        "outputId": "819b5b1e-6439-4abb-9aa0-8f58d6fc8c73"
      },
      "source": [
        "%%writefile \"/content/vectorize_script.py\"\n",
        "import hashlib\n",
        "import json\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Define the absolute path to the PDF file\n",
        "pdf_file_path = \"/content/BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf\"\n",
        "\n",
        "# 1. Load the PDF file\n",
        "if not os.path.exists(pdf_file_path):\n",
        "    print(f\"Error: PDF file not found at {pdf_file_path}\")\n",
        "    exit()\n",
        "\n",
        "loader = PyPDFLoader(pdf_file_path)\n",
        "docs = loader.load()\n",
        "\n",
        "# 2. Initialize RecursiveCharacterTextSplitter\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=850, # Updated to 850 for consistency\n",
        "    chunk_overlap=120 # Updated to 120 for consistency\n",
        ")\n",
        "\n",
        "# 3. Split the loaded PDF documents into chunks\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# 4. Create an empty list to store the processed chunks\n",
        "processed_chunks_list = []\n",
        "\n",
        "# 5. Iterate through each chunk, generate hash ID, and add metadata\n",
        "for i, chunk in enumerate(chunks):\n",
        "    # Generate a unique hash ID for the chunk.page_content\n",
        "    hash_id = hashlib.md5(chunk.page_content.encode('utf-8')).hexdigest()\n",
        "\n",
        "    # Create a dictionary for each processed chunk\n",
        "    processed_chunk = {\n",
        "        \"content\": chunk.page_content,\n",
        "        \"id\": hash_id,\n",
        "        \"metadata\": chunk.metadata\n",
        "    }\n",
        "\n",
        "    # Append this dictionary to your list of processed chunks\n",
        "    processed_chunks_list.append(processed_chunk)\n",
        "\n",
        "# 6. Save the list of processed chunks to a JSON file\n",
        "output_filename = \"processed_chunks.json\"\n",
        "with open(output_filename, \"w\") as f:\n",
        "    json.dump(processed_chunks_list, f, indent=2)\n",
        "\n",
        "print(f\"Processed {len(processed_chunks_list)} chunks and saved to {output_filename}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/vectorize_script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "554bf4b0",
        "outputId": "8a4b5a08-1928-49de-eb90-66bababaa947"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "# Execute the Python script\n",
        "result = subprocess.run(['python', '/content/vectorize (1).py'], capture_output=True, text=True)\n",
        "\n",
        "# Print the output from the script\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/vectorize (1).py\", line 3, in <module>\n",
            "    from langchain_community.document_loaders import PyPDFLoader\n",
            "ModuleNotFoundError: No module named 'langchain_community'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ba5acc51",
        "outputId": "75b13404-fc96-4a6b-887d-d1fc70915aac"
      },
      "source": [
        "pip install langchain-community langchain-text-splitters pypdf"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.6.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.2.6)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (0.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Downloading pypdf-6.6.0-py3-none-any.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-6.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9f827d6",
        "outputId": "e559cbd3-216b-4a50-dbcf-a8ce888a528c"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "# Execute the Python script\n",
        "result = subprocess.run(['python', '/content/vectorize (1).py'], capture_output=True, text=True)\n",
        "\n",
        "# Print the output from the script\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2026-01-21 07:31:55.995535: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768980716.014966    1736 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768980716.021156    1736 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768980716.036390    1736 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768980716.036414    1736 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768980716.036418    1736 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768980716.036420    1736 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 07:31:56.041040: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/vectorize (1).py\", line 7, in <module>\n",
            "    loader = PyPDFLoader(\"BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf\")\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_community/document_loaders/pdf.py\", line 281, in __init__\n",
            "    super().__init__(file_path, headers=headers)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_community/document_loaders/pdf.py\", line 140, in __init__\n",
            "    raise ValueError(\"File path %s is not a valid file or url\" % self.file_path)\n",
            "ValueError: File path BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf is not a valid file or url\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28910b7b",
        "outputId": "f44d9302-2474-4613-8ab4-65cbf92e28cb"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "# Execute the Python script\n",
        "result = subprocess.run(['python', '/content/vectorize_script.py'], capture_output=True, text=True)\n",
        "\n",
        "# Print the output from the script\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1461 chunks and saved to processed_chunks.json\n",
            "\n",
            "2026-01-21 08:53:48.071048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768985628.091137    5766 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768985628.097190    5766 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768985628.112260    5766 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768985628.112284    5766 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768985628.112288    5766 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768985628.112290    5766 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 08:53:48.116896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0379afa2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error indicated that the PDF file was not found at the specified path within the script. Before attempting another execution, I need to explicitly check if the PDF file exists in the `/content/` directory where the script expects it to be. This will help confirm the file's presence and rule out any path-related issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06cf3ee6",
        "outputId": "4db05897-25f0-4323-d284-49199727d097"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the absolute path to the PDF file\n",
        "pdf_file_path = \"/content/BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf\"\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(pdf_file_path):\n",
        "    print(f\"Success: PDF file found at {pdf_file_path}\")\n",
        "else:\n",
        "    print(f\"Error: PDF file NOT found at {pdf_file_path}. Please ensure the file is uploaded to the /content/ directory.\")\n",
        "\n",
        "# Optionally, list content of /content/ to see all files\n",
        "print(\"\\nContents of /content/ directory:\")\n",
        "!ls -l /content/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success: PDF file found at /content/BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf\n",
            "\n",
            "Contents of /content/ directory:\n",
            "total 9712\n",
            "-rw-r--r-- 1 root root 9934679 Jan 21 08:43 'BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf'\n",
            "drwxr-xr-x 1 root root    4096 Dec  9 14:42  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb417481",
        "outputId": "8b1417e3-8a9e-4166-aa1a-181bee31d077"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "pdf_filename = \"BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf\"\n",
        "source_path = f\"/{pdf_filename}\" # Assuming it might be in the root directory\n",
        "ddestination_path = f\"/content/{pdf_filename}\"\n",
        "\n",
        "if os.path.exists(source_path):\n",
        "    print(f\"PDF file found at {source_path}. Moving to {ddestination_path}...\")\n",
        "    shutil.move(source_path, ddestination_path)\n",
        "    print(\"File moved successfully.\")\n",
        "elif os.path.exists(ddestination_path):\n",
        "    print(f\"PDF file already present at {ddestination_path}.\")\n",
        "else:\n",
        "    print(f\"Error: PDF file NOT found at {source_path} or {ddestination_path}. Please upload '{pdf_filename}' to the `/content/` directory.\")\n",
        "    # If the file is not found, the script execution will fail again, so we'll stop here\n",
        "    # and let the next step (which checks for script output) reflect this.\n",
        "\n",
        "# Re-list content of /content/ to verify (optional, for debugging)\n",
        "print(\"\\nContents of /content/ directory after check/move:\")\n",
        "!ls -l /content/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: PDF file NOT found at /BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf or /content/BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf. Please upload 'BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf' to the `/content/` directory.\n",
            "\n",
            "Contents of /content/ directory after check/move:\n",
            "total 8\n",
            "drwxr-xr-x 1 root root 4096 Dec  9 14:42  sample_data\n",
            "-rw-r--r-- 1 root root 1595 Jan 21 07:32 'vectorize (1).py'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93132c68",
        "outputId": "e950a45e-d1ba-451f-f95e-8886a5b2c358"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "# Execute the Python script\n",
        "result = subprocess.run(['python', '/content/vectorize (1).py'], capture_output=True, text=True)\n",
        "\n",
        "# Print the output from the script\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: PDF file not found at /content/BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf\n",
            "\n",
            "2026-01-21 07:34:36.888205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768980876.908796    2454 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768980876.915093    2454 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768980876.930771    2454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768980876.930800    2454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768980876.930803    2454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768980876.930806    2454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 07:34:36.935428: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fd37140",
        "outputId": "e3824d84-7dde-4d78-89fe-99706add97e6"
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "pdf_file_path = \"/content/BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf\"\n",
        "\n",
        "# 1. Run the os.path.exists() check for the PDF file\n",
        "if os.path.exists(pdf_file_path):\n",
        "    print(f\"Success: PDF file found at {pdf_file_path}. Proceeding to execute the script.\")\n",
        "    # 2. If the PDF file is confirmed to be present, execute the Python script\n",
        "    result = subprocess.run(['python', '/content/vectorize (1).py'], capture_output=True, text=True)\n",
        "\n",
        "    # Print the output from the script\n",
        "    print(\"\\nScript stdout:\")\n",
        "    print(result.stdout)\n",
        "    print(\"\\nScript stderr:\")\n",
        "    print(result.stderr)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(\"Script executed successfully and processed_chunks.json should be created.\")\n",
        "    else:\n",
        "        print(\"Script execution failed. Check stderr for details.\")\n",
        "else:\n",
        "    print(f\"Error: PDF file NOT found at {pdf_file_path}. Please ensure the file is uploaded to the /content/ directory before proceeding.\")\n",
        "    print(\"Contents of /content/ directory:\")\n",
        "    !ls -l /content/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: PDF file NOT found at /content/BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf. Please ensure the file is uploaded to the /content/ directory before proceeding.\n",
            "Contents of /content/ directory:\n",
            "total 8\n",
            "drwxr-xr-x 1 root root 4096 Dec  9 14:42  sample_data\n",
            "-rw-r--r-- 1 root root 1595 Jan 21 07:32 'vectorize (1).py'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d266eca",
        "outputId": "12f9c38b-7d19-47cd-d111-0f01d8512b8c"
      },
      "source": [
        "%%writefile /content/generate_qa_dataset.py\n",
        "import json\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Securely set your Cerebras AI API key using Colab's secrets management\n",
        "# This part assumes the key is already set in the Colab notebook environment previously.\n",
        "# For script execution, it's typically read from environment variables or passed.\n",
        "# Re-initialize the LLM, prompt, and chain exactly as defined in cell cuLeLIs3BhiS for consistency.\n",
        "\n",
        "# IMPORTANT: Make sure os.environ[\"CEREBRAS_AI_KEY\"] is set in the main notebook environment\n",
        "# before running this script if you execute it via subprocess.\n",
        "# For direct execution within Colab, userdata.get() is fine, but subprocess needs os.environ.\n",
        "# We will pass the env variable explicitly in the calling cell.\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-oss-120b\", # Consistent with cuLeLIs3BhiS\n",
        "    base_url=\"https://api.cerebras.ai/v1\",\n",
        "    api_key=os.environ.get(\"CEREBRAS_AI_KEY\") # Use .get() for safety in scripts\n",
        ")\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=\"\"\"Your task is to generate one factual question and its ground truth answer STRICTLY from the provided context. Do NOT hallucinate. Ensure the question and answer are verbose, detailed, and directly contextual to the provided text. Ignore any non-factual filler like '----------------------------------'. Return the output as a JSON object with the following keys: 'question', 'contexts', 'answer', and 'ground_truth'.\n",
        "- 'question' should be a well-formulated, verbose question based on the context.\n",
        "- 'contexts' should be a list containing the provided context.\n",
        "- 'answer' should be a comprehensive, detailed ground truth answer, directly extracted or summarized from the context.\n",
        "- 'ground_truth' should be the same as 'answer', representing the comprehensive ground truth.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Return JSON:\n",
        "\n",
        "{{\n",
        "  \"question\": \"...\",\n",
        "  \"contexts\": [...],\n",
        "  \"answer\": \"...\",\n",
        "  \"ground_truth\": \"...\"\n",
        "}}\"\"\"\n",
        ")\n",
        "\n",
        "chain = prompt | llm | JsonOutputParser()\n",
        "\n",
        "# Load the processed_chunks.json file\n",
        "processed_chunks_file = \"processed_chunks.json\"\n",
        "if not os.path.exists(processed_chunks_file):\n",
        "    print(f\"Error: {processed_chunks_file} not found. Please run the chunk processing script first.\")\n",
        "    exit()\n",
        "\n",
        "with open(processed_chunks_file, \"r\") as f:\n",
        "    processed_chunks = json.load(f)\n",
        "\n",
        "# Create an empty list to store the generated Q&A dataset\n",
        "qna_dataset = []\n",
        "\n",
        "# Iterate through the loaded chunks, grouping them into batches of 5\n",
        "batch_size = 5\n",
        "max_batches = 100 # Limit to 100 batches to generate 100 Q&A pairs (100 * 5 = 500 chunks)\n",
        "\n",
        "for i in range(0, min(len(processed_chunks), max_batches * batch_size), batch_size):\n",
        "    batch = processed_chunks[i : i + batch_size]\n",
        "\n",
        "    # Combine the content of the chunks into a single context string\n",
        "    combined_context_content = \"\\n---\\n\".join([chunk[\"content\"] for chunk in batch])\n",
        "    batch_chunk_ids = [chunk[\"id\"] for chunk in batch]\n",
        "\n",
        "    try:\n",
        "        # Invoke the chain to generate a question-answer pair\n",
        "        qa_pair = chain.invoke({\"context\": combined_context_content})\n",
        "\n",
        "        # From the LLM's JSON output, extract the 'question' and 'answer'\n",
        "        # and construct a new dictionary for each generated Q&A pair.\n",
        "        qa_entry = {\n",
        "            \"question\": qa_pair.get(\"question\"),\n",
        "            \"answer\": qa_pair.get(\"answer\"),\n",
        "            \"source_chunk_ids\": batch_chunk_ids, # List of IDs from the 5 source chunks\n",
        "            \"contexts\": qa_pair.get(\"contexts\"),\n",
        "            \"ground_truth\": qa_pair.get(\"ground_truth\")\n",
        "        }\n",
        "        qna_dataset.append(qa_entry)\n",
        "        print(f\"Successfully generated Q&A for batch {i // batch_size + 1}/{max_batches}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating Q&A for batch {i // batch_size + 1}: {e}\")\n",
        "\n",
        "# Save the accumulated Q&A dataset list to a new JSON file\n",
        "output_qna_file = \"ragas_qa_dataset.json\"\n",
        "with open(output_qna_file, \"w\") as f:\n",
        "    json.dump(qna_dataset, f, indent=2)\n",
        "\n",
        "print(f\"Generated {len(qna_dataset)} Q&A pairs and saved to {output_qna_file}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/generate_qa_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "168da0cc",
        "outputId": "830cc771-aa89-4911-cac6-308388db71d2"
      },
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Ensure the CEREBRAS_AI_KEY is available as an environment variable for the script\n",
        "# This step assumes it has been set in the Colab notebook environment previously.\n",
        "# If not, the script will likely fail due to a missing API key.\n",
        "\n",
        "# Execute the Python script\n",
        "result = subprocess.run(['python', '/content/generate_qa_dataset.py'], capture_output=True, text=True)\n",
        "\n",
        "# Print the output from the script\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/generate_qa_dataset.py\", line 3, in <module>\n",
            "    from langchain_openai import ChatOpenAI\n",
            "ModuleNotFoundError: No module named 'langchain_openai'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a20ad72f",
        "outputId": "a4633723-ddb1-408c-91e2-95206ccdb103"
      },
      "source": [
        "pip install langchain-openai"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.6 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.2.6)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.14.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.13.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-1.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "543f8b17",
        "outputId": "d733d7df-f915-4d6e-ec65-5894bc3fa468"
      },
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Ensure the CEREBRAS_AI_KEY is available as an environment variable for the script\n",
        "# This step assumes it has been set in the Colab notebook environment previously.\n",
        "# If not, the script will likely fail due to a missing API key.\n",
        "\n",
        "# Execute the Python script\n",
        "result = subprocess.run(['python', '/content/generate_qa_dataset.py'], capture_output=True, text=True)\n",
        "\n",
        "# Print the output from the script\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/generate_qa_dataset.py\", line 19, in <module>\n",
            "    llm = ChatOpenAI(\n",
            "          ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/load/serializable.py\", line 117, in __init__\n",
            "    super().__init__(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pydantic/main.py\", line 250, in __init__\n",
            "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\", line 996, in validate_environment\n",
            "    self.root_async_client = openai.AsyncOpenAI(\n",
            "                             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_client.py\", line 488, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b531e97c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because the `CEREBRAS_AI_KEY` was not properly set as an environment variable for the subprocess. I need to ensure the `CEREBRAS_AI_KEY` from `userdata` is explicitly passed as an environment variable to the `subprocess.run` command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5e1ca6f",
        "outputId": "cf58c0b6-41d1-4894-c56a-80395f199f64"
      },
      "source": [
        "import subprocess\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the Cerebras AI key from Colab's user data\n",
        "cerebras_api_key = userdata.get(\"CEREBRAS_AI_KEY\")\n",
        "\n",
        "# Prepare the environment variables for the subprocess\n",
        "env = os.environ.copy()\n",
        "env[\"CEREBRAS_AI_KEY\"] = cerebras_api_key\n",
        "\n",
        "# Execute the Python script, passing the environment variables\n",
        "result = subprocess.run(\n",
        "    ['python', '/content/generate_qa_dataset.py'],\n",
        "    capture_output=True,\n",
        "    text=True,\n",
        "    env=env\n",
        ")\n",
        "\n",
        "# Print the output from the script\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully generated Q&A for batch 1/100\n",
            "Successfully generated Q&A for batch 2/100\n",
            "Successfully generated Q&A for batch 3/100\n",
            "Successfully generated Q&A for batch 4/100\n",
            "Successfully generated Q&A for batch 5/100\n",
            "Successfully generated Q&A for batch 6/100\n",
            "Successfully generated Q&A for batch 7/100\n",
            "Successfully generated Q&A for batch 8/100\n",
            "Successfully generated Q&A for batch 9/100\n",
            "Successfully generated Q&A for batch 10/100\n",
            "Successfully generated Q&A for batch 11/100\n",
            "Successfully generated Q&A for batch 12/100\n",
            "Successfully generated Q&A for batch 13/100\n",
            "Successfully generated Q&A for batch 14/100\n",
            "Successfully generated Q&A for batch 15/100\n",
            "Successfully generated Q&A for batch 16/100\n",
            "Successfully generated Q&A for batch 17/100\n",
            "Successfully generated Q&A for batch 18/100\n",
            "Successfully generated Q&A for batch 19/100\n",
            "Successfully generated Q&A for batch 20/100\n",
            "Successfully generated Q&A for batch 21/100\n",
            "Successfully generated Q&A for batch 22/100\n",
            "Successfully generated Q&A for batch 23/100\n",
            "Successfully generated Q&A for batch 24/100\n",
            "Successfully generated Q&A for batch 25/100\n",
            "Successfully generated Q&A for batch 26/100\n",
            "Successfully generated Q&A for batch 27/100\n",
            "Successfully generated Q&A for batch 28/100\n",
            "Successfully generated Q&A for batch 29/100\n",
            "Successfully generated Q&A for batch 30/100\n",
            "Successfully generated Q&A for batch 31/100\n",
            "Successfully generated Q&A for batch 32/100\n",
            "Successfully generated Q&A for batch 33/100\n",
            "Successfully generated Q&A for batch 34/100\n",
            "Successfully generated Q&A for batch 35/100\n",
            "Successfully generated Q&A for batch 36/100\n",
            "Successfully generated Q&A for batch 37/100\n",
            "Successfully generated Q&A for batch 38/100\n",
            "Successfully generated Q&A for batch 39/100\n",
            "Successfully generated Q&A for batch 40/100\n",
            "Successfully generated Q&A for batch 41/100\n",
            "Successfully generated Q&A for batch 42/100\n",
            "Successfully generated Q&A for batch 43/100\n",
            "Successfully generated Q&A for batch 44/100\n",
            "Successfully generated Q&A for batch 45/100\n",
            "Successfully generated Q&A for batch 46/100\n",
            "Successfully generated Q&A for batch 47/100\n",
            "Successfully generated Q&A for batch 48/100\n",
            "Successfully generated Q&A for batch 49/100\n",
            "Successfully generated Q&A for batch 50/100\n",
            "Successfully generated Q&A for batch 51/100\n",
            "Successfully generated Q&A for batch 52/100\n",
            "Successfully generated Q&A for batch 53/100\n",
            "Successfully generated Q&A for batch 54/100\n",
            "Successfully generated Q&A for batch 55/100\n",
            "Successfully generated Q&A for batch 56/100\n",
            "Successfully generated Q&A for batch 57/100\n",
            "Successfully generated Q&A for batch 58/100\n",
            "Successfully generated Q&A for batch 59/100\n",
            "Successfully generated Q&A for batch 60/100\n",
            "Successfully generated Q&A for batch 61/100\n",
            "Successfully generated Q&A for batch 62/100\n",
            "Successfully generated Q&A for batch 63/100\n",
            "Successfully generated Q&A for batch 64/100\n",
            "Successfully generated Q&A for batch 65/100\n",
            "Successfully generated Q&A for batch 66/100\n",
            "Successfully generated Q&A for batch 67/100\n",
            "Successfully generated Q&A for batch 68/100\n",
            "Successfully generated Q&A for batch 69/100\n",
            "Successfully generated Q&A for batch 70/100\n",
            "Successfully generated Q&A for batch 71/100\n",
            "Successfully generated Q&A for batch 72/100\n",
            "Successfully generated Q&A for batch 73/100\n",
            "Successfully generated Q&A for batch 74/100\n",
            "Successfully generated Q&A for batch 75/100\n",
            "Error generating Q&A for batch 76: 'NoneType' object has no attribute 'get'\n",
            "Successfully generated Q&A for batch 77/100\n",
            "Successfully generated Q&A for batch 78/100\n",
            "Successfully generated Q&A for batch 79/100\n",
            "Successfully generated Q&A for batch 80/100\n",
            "Successfully generated Q&A for batch 81/100\n",
            "Successfully generated Q&A for batch 82/100\n",
            "Successfully generated Q&A for batch 83/100\n",
            "Successfully generated Q&A for batch 84/100\n",
            "Successfully generated Q&A for batch 85/100\n",
            "Successfully generated Q&A for batch 86/100\n",
            "Successfully generated Q&A for batch 87/100\n",
            "Successfully generated Q&A for batch 88/100\n",
            "Successfully generated Q&A for batch 89/100\n",
            "Successfully generated Q&A for batch 90/100\n",
            "Successfully generated Q&A for batch 91/100\n",
            "Successfully generated Q&A for batch 92/100\n",
            "Successfully generated Q&A for batch 93/100\n",
            "Error generating Q&A for batch 94: Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "Successfully generated Q&A for batch 95/100\n",
            "Successfully generated Q&A for batch 96/100\n",
            "Successfully generated Q&A for batch 97/100\n",
            "Successfully generated Q&A for batch 98/100\n",
            "Successfully generated Q&A for batch 99/100\n",
            "Successfully generated Q&A for batch 100/100\n",
            "Generated 98 Q&A pairs and saved to ragas_qa_dataset.json\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cab80a73",
        "outputId": "87c79582-8027-4b39-d781-c5e35af03e99"
      },
      "source": [
        "pip install sentence-transformers faiss-cpu"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
            "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "399c3bb0",
        "outputId": "4b86dd00-a73a-40fe-eded-0962f3e97bef"
      },
      "source": [
        "%%writefile /content/embed_chunks.py\n",
        "import json\n",
        "import os\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# 1. Define the path to the processed chunks file\n",
        "processed_chunks_file = \"processed_chunks.json\"\n",
        "\n",
        "# 2. Check if the processed_chunks.json file exists\n",
        "if not os.path.exists(processed_chunks_file):\n",
        "    print(f\"Error: {processed_chunks_file} not found. Please ensure the chunk processing script was run successfully.\")\n",
        "    exit()\n",
        "\n",
        "# 3. Load the processed chunks\n",
        "with open(processed_chunks_file, \"r\") as f:\n",
        "    processed_chunks_data = json.load(f)\n",
        "\n",
        "# 4. Initialize an embedding model\n",
        "# Using 'BAAI/bge-small-en-v1.5' for efficiency and good performance\n",
        "embeddings = HuggingFaceBgeEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
        "\n",
        "# 5. Prepare Document objects for vector store\n",
        "# Each Document object should contain page_content and metadata\n",
        "documents_for_vectorstore = []\n",
        "for chunk in processed_chunks_data:\n",
        "    # Ensure metadata includes the original chunk's id and any other metadata\n",
        "    doc_metadata = chunk.get(\"metadata\", {})\n",
        "    doc_metadata[\"id\"] = chunk.get(\"id\") # Add the hash ID to metadata for retrieval\n",
        "\n",
        "    doc = Document(\n",
        "        page_content=chunk.get(\"content\"),\n",
        "        metadata=doc_metadata\n",
        "    )\n",
        "    documents_for_vectorstore.append(doc)\n",
        "\n",
        "# 6. Create an in-memory FAISS vector store\n",
        "print(\"Creating FAISS vector store from documents...\")\n",
        "vectorstore = FAISS.from_documents(documents_for_vectorstore, embeddings)\n",
        "print(f\"Successfully created FAISS vector store with {len(documents_for_vectorstore)} documents.\")\n",
        "\n",
        "# You can optionally save the vector store to disk if needed for persistence\n",
        "# vectorstore.save_local(\"faiss_index\")\n",
        "# print(\"FAISS index saved locally.\")\n",
        "\n",
        "print(\"Embedding process complete and in-memory FAISS vector store created.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/embed_chunks.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e032f77e",
        "outputId": "39309af5-ad5e-4ac8-e628-fc8586aaf308"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "# Execute the Python script\n",
        "result = subprocess.run(['python', '/content/embed_chunks.py'], capture_output=True, text=True)\n",
        "\n",
        "# Print the output from the script\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating FAISS vector store from documents...\n",
            "Successfully created FAISS vector store with 1461 documents.\n",
            "Embedding process complete and in-memory FAISS vector store created.\n",
            "\n",
            "/content/embed_chunks.py:21: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceBgeEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
            "2026-01-21 09:02:55.048867: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768986175.068398    8172 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768986175.074357    8172 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768986175.089618    8172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768986175.089641    8172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768986175.089644    8172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768986175.089646    8172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 09:02:55.094165: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0522499b",
        "outputId": "1f4ed947-2f0f-40c5-92ec-4a0f28d06937"
      },
      "source": [
        "%%writefile /content/embed_chunks.py\n",
        "import json\n",
        "import os\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# 1. Define the path to the processed chunks file\n",
        "processed_chunks_file = \"processed_chunks.json\"\n",
        "\n",
        "# 2. Check if the processed_chunks.json file exists\n",
        "if not os.path.exists(processed_chunks_file):\n",
        "    print(f\"Error: {processed_chunks_file} not found. Please ensure the chunk processing script was run successfully.\")\n",
        "    exit()\n",
        "\n",
        "# 3. Load the processed chunks\n",
        "with open(processed_chunks_file, \"r\") as f:\n",
        "    processed_chunks_data = json.load(f)\n",
        "\n",
        "# 4. Initialize an embedding model\n",
        "# Using 'BAAI/bge-small-en-v1.5' for efficiency and good performance\n",
        "embeddings = HuggingFaceBgeEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
        "\n",
        "# 5. Prepare Document objects for vector store\n",
        "# Each Document object should contain page_content and metadata\n",
        "documents_for_vectorstore = []\n",
        "for chunk in processed_chunks_data:\n",
        "    # Ensure metadata includes the original chunk's id and any other metadata\n",
        "    doc_metadata = chunk.get(\"metadata\", {})\n",
        "    doc_metadata[\"id\"] = chunk.get(\"id\") # Add the hash ID to metadata for retrieval\n",
        "\n",
        "    doc = Document(\n",
        "        page_content=chunk.get(\"content\"),\n",
        "        metadata=doc_metadata\n",
        "    )\n",
        "    documents_for_vectorstore.append(doc)\n",
        "\n",
        "# 6. Create an in-memory FAISS vector store\n",
        "print(\"Creating FAISS vector store from documents...\")\n",
        "vectorstore = FAISS.from_documents(documents_for_vectorstore, embeddings)\n",
        "print(f\"Successfully created FAISS vector store with {len(documents_for_vectorstore)} documents.\")\n",
        "\n",
        "# --- Verification Step ---\n",
        "print(\"\\n--- Verification: Performing a sample retrieval query ---\")\n",
        "\n",
        "# Define a sample query string\n",
        "query = \"What are the steps for pre-underway checks?\"\n",
        "\n",
        "# Perform a similarity search\n",
        "retrieved_docs = vectorstore.similarity_search(query, k=3) # Retrieve top 3 relevant chunks\n",
        "\n",
        "print(f\"Query: '{query}'\")\n",
        "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
        "\n",
        "# Iterate through retrieved documents and print their content and hash IDs\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(f\"Content (first 200 chars): {doc.page_content[:200]}...\")\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "    print(f\"Hash ID from metadata: {doc.metadata.get('id', 'N/A')}\")\n",
        "\n",
        "print(\"\\nVerification complete.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/embed_chunks.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db45cce9",
        "outputId": "9f510b18-b9b1-44a9-db2b-385d9e07d1cb"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "# Execute the Python script\n",
        "result = subprocess.run(['python', '/content/embed_chunks.py'], capture_output=True, text=True)\n",
        "\n",
        "# Print the output from the script\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating FAISS vector store from documents...\n",
            "Successfully created FAISS vector store with 1461 documents.\n",
            "\n",
            "--- Verification: Performing a sample retrieval query ---\n",
            "Query: 'What are the steps for pre-underway checks?'\n",
            "Retrieved 3 documents:\n",
            "\n",
            "Document 1:\n",
            "Content (first 200 chars): D.2. Before ApproachingBefore beginning the approach to the disabled vessel, the coxswain must ensure the crew and boat are prepared as follows: Step Procedure 1 Make engine room rounds prior to begin...\n",
            "Metadata: {'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2022-11-02T08:05:45-04:00', 'author': 'TRPark2', 'moddate': '2022-11-02T08:05:45-04:00', 'title': 'BCH1B.pdf', 'source': '/content/BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf', 'total_pages': 293, 'page': 125, 'page_label': '126', 'id': 'cdaf720ba38668331f486539fda9ea5d'}\n",
            "Hash ID from metadata: cdaf720ba38668331f486539fda9ea5d\n",
            "\n",
            "Document 2:\n",
            "Content (first 200 chars): NOTE NOTE D.3. GuidelinesThe following guidelines must be used to stand a proper lookout watch:(01)Remain alert and give full attention to the assigned duty.(02)Remain at station until relieved.(03)Do...\n",
            "Metadata: {'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2022-11-02T08:05:45-04:00', 'author': 'TRPark2', 'moddate': '2022-11-02T08:05:45-04:00', 'title': 'BCH1B.pdf', 'source': '/content/BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf', 'total_pages': 293, 'page': 28, 'page_label': '29', 'id': 'd4d46c1a1671bfc2ee4004756756f5df'}\n",
            "Hash ID from metadata: d4d46c1a1671bfc2ee4004756756f5df\n",
            "\n",
            "Document 3:\n",
            "Content (first 200 chars): the distressed vessel‚Äôs crew thinks is important before arriving on scene (lines or gear in the water, nearby vessels, etc.).5 Determine if anything has changed since the distressed vessel‚Äôs last cont...\n",
            "Metadata: {'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2022-11-02T08:05:45-04:00', 'author': 'TRPark2', 'moddate': '2022-11-02T08:05:45-04:00', 'title': 'BCH1B.pdf', 'source': '/content/BOAT CREW HANDBOOK - 16114.1B_Boat Operations.pdf', 'total_pages': 293, 'page': 80, 'page_label': '81', 'id': '1bd1407294e774d3a5f1940ca39381b9'}\n",
            "Hash ID from metadata: 1bd1407294e774d3a5f1940ca39381b9\n",
            "\n",
            "Verification complete.\n",
            "\n",
            "/content/embed_chunks.py:21: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceBgeEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
            "2026-01-21 09:04:06.917110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768986246.937705    8520 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768986246.943946    8520 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768986246.958938    8520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768986246.958960    8520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768986246.958963    8520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768986246.958965    8520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 09:04:06.963383: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}